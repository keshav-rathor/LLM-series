{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Data Parallelism**\nData Parallelism (DP) replicates the model across multiple GPUs. Data batches are evenly distributed between GPUs and the data-parallel GPUs process them independently. While the computation workload is efficiently distributed across GPUs, inter-GPU communication is required in order to keep the model replicas consistent between training steps.","metadata":{}},{"cell_type":"markdown","source":"# **Distributed Data Parallelism**\nDistributed Data Parallelism (DDP) keeps the model copies consistent by synchronizing parameter gradients across data-parallel GPUs before each parameter update. More specifically, it sums the gradients of all model copies using all-reduce communication collectives.","metadata":{}},{"cell_type":"markdown","source":"---\n\n## üß© How Data Parallelism Synchronizes Gradients in PyTorch\n\nWhen training deep learning models on large datasets, it‚Äôs common to split the workload across multiple GPUs to speed up training. This is known as **data parallelism**. In PyTorch, the most common tool for this is `torch.nn.DataParallel` or, more efficiently, `torch.nn.parallel.DistributedDataParallel` (DDP).\nLet‚Äôs break down what actually happens under the hood, focusing on how gradients are synchronized.\n\n---\n\n### ‚úÖ Step 1: Replicating the Model\n\nAt the start of training, PyTorch copies (replicates) the same model onto each GPU.\nEach GPU holds a complete copy of the model‚Äôs parameters and optimizer state.\n\n---\n\n### ‚úÖ Step 2: Splitting the Batch\n\nWhen a batch of input data arrives, it is **split into equal chunks** (or nearly equal if the batch size isn‚Äôt divisible by the number of GPUs).\nFor example, with a batch size of 128 and 4 GPUs, each GPU processes a chunk of 32 samples.\n\n---\n\n### ‚úÖ Step 3: Local Forward and Backward Pass\n\nEach GPU runs:\n\n* The **forward pass** on its chunk of data, computing local outputs and loss.\n* The **backward pass** to compute **local gradients** with respect to its chunk.\n\nAt this point, each GPU has its own gradients ‚Äî but these gradients differ, since each GPU only saw part of the batch.\n\n---\n\n### üîÑ Step 4: Gradient Synchronization\n\nHere is the key step that keeps all GPUs in sync:\n\n* During the backward pass, PyTorch automatically performs an **all-reduce** operation across all GPUs.\n* All-reduce aggregates (typically sums) the gradients from each GPU and distributes the result back to every GPU.\n* Each GPU then divides the summed gradients by the number of GPUs to get the **averaged gradients**.\n\nAs a result, **every GPU ends up with identical gradients**, as if the model had processed the entire batch on a single device.\n\n---\n\n### ‚úÖ Step 5: Optimizer Step\n\nFinally, each GPU independently applies the optimizer step using these synchronized gradients.\nBecause the gradients are identical across GPUs, the model parameters remain in sync across all GPUs after each step.\n\n---\n\n## ‚ö° Why this matters\n\nThis strategy ensures that:\n\n* Training behaves the same as if it were running on a single GPU with a larger batch.\n* Each GPU does useful work on different parts of the data, and the gradient synchronization step ensures that the models do **not** drift apart.\n\n---\n\n## üõ† Distributed Optimizer & Sharded Training (briefly)\n\nFor very large models (like LLMs), keeping a full copy of the model and optimizer states on each GPU becomes infeasible.\nTo solve this, frameworks like **Megatron-LM** and **DeepSpeed** implement **optimizer state sharding**:\n\n* Instead of every GPU keeping all optimizer states, each GPU keeps only a shard (subset).\n* Gradients are reduce-scattered (so each GPU only gets the part it needs).\n* Updated parameter shards are all-gathered to keep parameters in sync.\n\nThis approach significantly reduces memory use, enabling training of much larger models.\n\n---\n\n## ‚úÖ Summary\n\nIn PyTorch‚Äôs data parallelism:\n\n* Each GPU computes gradients locally.\n* PyTorch automatically **synchronizes** these gradients across GPUs via all-reduce.\n* The optimizer step then updates parameters identically on each GPU, keeping the models in sync.\n\nThis pattern is efficient, scales well across multiple GPUs, and is the foundation of most multi-GPU training in PyTorch.\n\n---\n\nIf you'd like, I can now help you:\n‚úÖ Adapt your code to show this with a simple neural network and random data,\n‚úÖ Add diagrams or code comments to illustrate these steps.\n\nJust send your code when you're ready! üöÄ\n","metadata":{}},{"cell_type":"markdown","source":"\n‚úÖ **Step 1:** You move your model to the GPU\n\n```python\ndevice = torch.device(\"cuda:0\")\nmodel.to(device)\n```\n\nThis tells PyTorch:\n\n> ‚ÄúI want to run this model on GPU #0.‚Äù\n\nAll parameters and buffers inside your model are moved to that GPU.\n\n---\n\n‚úÖ **Step 2:** You wrap your model in `torch.nn.DataParallel`\n\n```python\nmodel = nn.DataParallel(model)\n```\n\nThis makes your model **use all available GPUs automatically**.\n\nWhat actually happens:\n\n* `DataParallel` keeps the *original* model on the first GPU (`cuda:0`).\n* During training or inference, it splits each input batch into chunks (by default, equally).\n* Each chunk is sent to a different GPU (if you have, e.g., 2 GPUs, half the batch to GPU 0, half to GPU 1).\n* Each GPU runs the forward and backward pass on its chunk.\n* The gradients are gathered and averaged on GPU 0.\n* The optimizer then updates the parameters (which live on GPU 0).\n\n---\n\n‚ö† **Important:**\nWhen you move your data (inputs, labels) to device, you still use:\n\n```python\ninputs = inputs.to(device)\n```\n\nwhere `device` is still `\"cuda:0\"`.\n`DataParallel` handles splitting automatically once you call the model.\n\n---\n\n‚ú® **Summary (in words):**\n\n* `.to(device)` ‚Üí puts the model on GPU #0\n* `DataParallel` ‚Üí makes it so all GPUs are used, by splitting the batch and gathering results\n* You don‚Äôt change your training loop much ‚Äî just wrap the model and keep sending data to device.\n\n---\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Imports and parameters**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Parameters and DataLoaders\ninput_size = 5\noutput_size = 2\n\nbatch_size = 30\ndata_size = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:19:33.097500Z","iopub.execute_input":"2025-07-07T11:19:33.097781Z","iopub.status.idle":"2025-07-07T11:19:38.227898Z","shell.execute_reply.started":"2025-07-07T11:19:33.097749Z","shell.execute_reply":"2025-07-07T11:19:38.226971Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Your device is: \",device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T06:40:14.926570Z","iopub.execute_input":"2025-07-07T06:40:14.927095Z","iopub.status.idle":"2025-07-07T06:40:14.932068Z","shell.execute_reply.started":"2025-07-07T06:40:14.927065Z","shell.execute_reply":"2025-07-07T06:40:14.931016Z"}},"outputs":[{"name":"stdout","text":"Your device is:  cpu\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **Random Dataset**","metadata":{}},{"cell_type":"code","source":"class RandomDataset(Dataset):\n\n    def __init__(self, size, length):\n        self.len = length\n        self.data = torch.randn(length, size)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return self.len\n\nrand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n                         batch_size=batch_size, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Sample Model**\nFor this demo, our model simply takes an input, applies a linear transformation, and produces an output. However, you can apply DataParallel to any model architecture‚Äîwhether it‚Äôs a CNN, RNN, Capsule Network, or something else.\n\nWe‚Äôve added a print statement inside the model to show the size of the input and output tensors.\nWatch carefully what gets printed on the primary device (batch rank 0).","metadata":{}},{"cell_type":"code","source":"# 3-layer NN\nclass Model(nn.Module):\n    def __init__(self, input_size, hidden_size, hidden_size2, output_size):\n        super(Model, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size2),\n            nn.ReLU(),\n            nn.Linear(hidden_size2, output_size)\n        )\n\n    def forward(self, x):\n        out = self.net(x)\n        print(\"\\tIn Model: input size\", x.size(), \"output size\", out.size())\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T06:43:09.354013Z","iopub.execute_input":"2025-07-07T06:43:09.354317Z","iopub.status.idle":"2025-07-07T06:43:09.360026Z","shell.execute_reply.started":"2025-07-07T06:43:09.354293Z","shell.execute_reply":"2025-07-07T06:43:09.359049Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **Create Model and DataParallel**\nThis is the core part of the tutorial. First, we need to make a model instance and check if we have multiple GPUs. If we have multiple GPUs, we can wrap our model using nn.DataParallel. Then we can put our model on GPUs by model.to(device)","metadata":{}},{"cell_type":"code","source":"model = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T06:43:44.266357Z","iopub.execute_input":"2025-07-07T06:43:44.266691Z","iopub.status.idle":"2025-07-07T06:43:44.298739Z","shell.execute_reply.started":"2025-07-07T06:43:44.266660Z","shell.execute_reply":"2025-07-07T06:43:44.297784Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Model(\n  (fc): Linear(in_features=5, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Optimizer and loss---------------------------------\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Run the Model**\nNow we can see the sizes of input and output tensors.","metadata":{}},{"cell_type":"code","source":"# Training loop\nfor data in rand_loader:\n    inputs = data.to(device)\n\n    # Forward pass: each GPU handles a chunk\n    outputs = model(inputs)\n\n    # Dummy targets\n    targets = torch.randn(inputs.size(0), output_size).to(device)\n\n    # Compute loss\n    loss = criterion(outputs, targets)\n\n    # Zero old gradients\n    optimizer.zero_grad()\n\n    # Backward pass\n    loss.backward()\n    # At this point:\n    # - Each GPU computes gradients on its chunk.\n    # - PyTorch does all-reduce ‚Üí GPUs now have same averaged gradients.\n\n    # Optimizer step\n    optimizer.step()\n    # Each GPU updates its local weights identically ‚Üí all stay in sync.\n\n    print(\"Outside: input size\", inputs.size(), \"output_size\", outputs.size())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T06:44:35.008210Z","iopub.execute_input":"2025-07-07T06:44:35.009025Z","iopub.status.idle":"2025-07-07T06:44:35.082119Z","shell.execute_reply.started":"2025-07-07T06:44:35.008994Z","shell.execute_reply":"2025-07-07T06:44:35.081212Z"}},"outputs":[{"name":"stdout","text":"\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# **Results**\nIf you have no GPU or one GPU, when we batch 30 inputs and 30 outputs, the model gets 30 and outputs 30 as expected. But if you have multiple GPUs, then you can get results like this.\n\n2 GPUs\n\n\nIf you have 2, you will see:\n\n    # on 2 GPUs\n    Let's use 2 GPUs!\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n\n3 GPUs\nIf you have 3 GPUs, you will see:\n\n    Let's use 3 GPUs!\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n\n8 GPUs\nIf you have 8, you will see:\n\n\n    Let's use 8 GPUs!\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}